{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert scenarios form the 'docs' format to 'csv' format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"scenario1.csv\", \"scenario2.csv\", \"scenario3.csv\", \"scenario4.csv\", \"scenario5.csv\", \"scenario6.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docx_to_csv(docx_path, csv_path):\n",
    "    # Load the DOCX file\n",
    "    doc = Document(docx_path)\n",
    "    \n",
    "    # Extract paragraphs\n",
    "    paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip() != \"\"]\n",
    "    \n",
    "    # Create a DataFrame with each paragraph as a column\n",
    "    df = pd.DataFrame(paragraphs[1:], columns=['Scenario'])\n",
    "    print(f\"{docx_path} :  size: {len(df)}\")\n",
    "\n",
    "    duplicated_rows = df[df.duplicated()]\n",
    "    if len(duplicated_rows):\n",
    "        print(\"Duplicated rows:\")\n",
    "        print(duplicated_rows)\n",
    "    # Save DataFrame to a CSV file\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_matching_extensions_str(file_name, extensions):\n",
    "    base_name = file_name\n",
    "    for ext in extensions:\n",
    "        if base_name.lower().endswith(ext.lower()):\n",
    "            base_name = base_name[: -len(ext)]\n",
    "    return base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove the \"_versions\" suffix from the filename\n",
    "def remove_versions_suffix(filename):\n",
    "    return filename.replace('_versions', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_files_to_csv(directory, extensions, output_directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(extensions):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                print(f\" File: {file_path}\")\n",
    "\n",
    "                docx_path = file_path\n",
    "                os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "                # base_name= remove_matching_extensions_str(file_name, extensions)\n",
    "                base_name, ext = os.path.splitext(file_name)\n",
    "                csv_path = output_directory+remove_versions_suffix(base_name)+\".csv\"\n",
    "                docx_to_csv(docx_path, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " File: ./scenarios/scenario3_versions.docx\n",
      "./scenarios/scenario3_versions.docx :  size: 40\n",
      " File: ./scenarios/scenario4_versions.docx\n",
      "./scenarios/scenario4_versions.docx :  size: 40\n",
      " File: ./scenarios/scenario5_versions.docx\n",
      "./scenarios/scenario5_versions.docx :  size: 40\n",
      " File: ./scenarios/scenario1_versions.docx\n",
      "./scenarios/scenario1_versions.docx :  size: 40\n",
      " File: ./scenarios/scenario6_versions.docx\n",
      "./scenarios/scenario6_versions.docx :  size: 40\n",
      " File: ./scenarios/scenario2_versions.docx\n",
      "./scenarios/scenario2_versions.docx :  size: 40\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory you want to list\n",
    "directory_path = './scenarios'\n",
    "output_directory = \"./scenarios_versions_csv_format/\"\n",
    "extensions = \".docx\"\n",
    "convert_files_to_csv(directory_path, extensions, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert document table to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tables_from_docx(docx_path):\n",
    "    # Load the DOCX file\n",
    "    print(docx_path)\n",
    "    doc = Document(docx_path)\n",
    "\n",
    "    tables = []\n",
    "    # Iterate over tables in the document\n",
    "    for table in doc.tables:\n",
    "        table_data = []\n",
    "        # Iterate over rows in the table\n",
    "        for row in table.rows:\n",
    "            row_data = [cell.text.strip() for cell in row.cells]\n",
    "            table_data.append(row_data)\n",
    "        tables.append(table_data)\n",
    "    return tables\n",
    "\n",
    "def convert_tables_to_dataframes(tables):\n",
    "    dataframes = []\n",
    "    for table in tables:\n",
    "        df = pd.DataFrame(table)\n",
    "        dataframes.append(df)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_tasks_output_table.docx\n"
     ]
    }
   ],
   "source": [
    "# Path to your DOCX file\n",
    "docx_path = \"multi_tasks_output_table.docx\"\n",
    "output_csv_path = os.path.splitext(docx_path)[0]+\".csv\"\n",
    "\n",
    "# Read tables from DOCX\n",
    "tables = read_tables_from_docx(docx_path)\n",
    "\n",
    "# Convert tables to DataFrames\n",
    "dataframes = convert_tables_to_dataframes(tables)\n",
    "\n",
    "# for the first tables\n",
    "df = dataframes[0]\n",
    "df = df.rename(columns={0: 'Tasks', 1: 'Classes Corresponding to Task'})\n",
    "df.to_csv(output_csv_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Columns name: Scenarios (X), Tasks Names (Lables) (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract column names\n",
    "columns_name = df.set_index(\"Tasks\").T.columns\n",
    "\n",
    "columns_name = [\"Scenario\"] + list(columns_name)\n",
    "output_path = \"dataset_columns_names.csv\"\n",
    "# Convert to DataFrame with a single row and save to CSV\n",
    "pd.DataFrame(columns = columns_name).to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct or Match the column name of the new dataset with the old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_in_column_names(path1):    \n",
    "    files1 = os.listdir(path1)\n",
    "    \n",
    "    # Filter CSV files if needed\n",
    "    csv_files1 = [os.path.join(path1,file) for file in files1 if file.endswith('.csv')]\n",
    "    print(csv_files1)\n",
    "    for file_path in csv_files1:\n",
    "        df1 = pd.read_csv(file_path)\n",
    "        if \"Sub - mission\" not in df1.columns:\n",
    "            df1.insert(loc=2, column='Sub - mission', value=\"\")\n",
    "        df1['Sub - mission'] = \"\"\n",
    "        rename_column_names = {'Hard Constraints': 'Hard Constrains' , 'Soft Constraints' : 'Soft Constrains (Preferences)'}\n",
    "        df1.rename(columns=rename_column_names, inplace=True)\n",
    "\n",
    "        # df1.drop(columns=['Unnamed: 0.1'], inplace=True)\n",
    "\n",
    "        df1.to_csv(file_path,index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['old_dataset/unlabelled/scenario4.csv', 'old_dataset/unlabelled/scenario5.csv', 'old_dataset/unlabelled/scenario6.csv', 'old_dataset/unlabelled/scenario2.csv', 'old_dataset/unlabelled/scenario3.csv', 'old_dataset/unlabelled/scenario1.csv']\n",
      "['old_dataset/labelled/scenario4.csv', 'old_dataset/labelled/scenario5.csv', 'old_dataset/labelled/scenario6.csv', 'old_dataset/labelled/scenario2.csv', 'old_dataset/labelled/scenario3.csv', 'old_dataset/labelled/scenario1.csv']\n",
      "['scenarios_examples_multi_tasks_output_labels/scenario4.csv', 'scenarios_examples_multi_tasks_output_labels/scenario5.csv', 'scenarios_examples_multi_tasks_output_labels/scenario6.csv', 'scenarios_examples_multi_tasks_output_labels/scenario2.csv', 'scenarios_examples_multi_tasks_output_labels/scenario3.csv', 'scenarios_examples_multi_tasks_output_labels/scenario1.csv']\n"
     ]
    }
   ],
   "source": [
    "path= \"scenarios_examples_multi_tasks_output_labels\"\n",
    "path_old_labelled = \"old_dataset/labelled/\"\n",
    "path_old_unlabelled = \"old_dataset/unlabelled/\"\n",
    "\n",
    "path_correct = \"dataset_columns_names.csv\"\n",
    "\n",
    "correction_in_column_names(path_old_unlabelled)\n",
    "correction_in_column_names(path_old_labelled)\n",
    "correction_in_column_names(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Assign labels from scenarios examples to scenarios versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_scenarios_examples_and_version(df1,df2):\n",
    "    columns_to_add = df1.columns[1:]\n",
    "    values_to_add = df1.iloc[0, 1:]\n",
    "\n",
    "    # Add the columns to df2\n",
    "    for col in columns_to_add:\n",
    "        df2[col] = values_to_add[col]\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels_from_scenarios_examples(path1, path2, output_directory):\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    files1 = os.listdir(path1)\n",
    "    files2 = os.listdir(path2)\n",
    "    # Filter CSV files if needed\n",
    "    csv_files1 = [file for file in files1 if file.endswith('.csv')]\n",
    "    \n",
    "    csv_files2 = [file for file in files2 if file.endswith('.csv')]\n",
    "    \n",
    "    for file1 in csv_files1:\n",
    "        for file2 in csv_files2:\n",
    "            if file1 == file2:\n",
    "                df1 = pd.read_csv(os.path.join(path1,file1))\n",
    "                df2 = pd.read_csv(os.path.join(path2,file2))\n",
    "                df = combine_scenarios_examples_and_version(df1,df2)\n",
    "                df.to_csv(os.path.join(output_directory, file2), index = False)\n",
    "\n",
    "    print(csv_files1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scenario4.csv', 'scenario5.csv', 'scenario6.csv', 'scenario2.csv', 'scenario3.csv', 'scenario1.csv']\n"
     ]
    }
   ],
   "source": [
    "scenarios_examples_path = \"./scenarios_examples_multi_tasks_output_labels\"\n",
    "scenarios_versions_path = \"./scenarios_versions_csv_format\"\n",
    "\n",
    "scenarios_with_labels = \"./scenarios_with_labels\"\n",
    "\n",
    "assign_labels_from_scenarios_examples(scenarios_examples_path, scenarios_versions_path, scenarios_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all the scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_scenarios(path):\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    # Filter CSV files if needed\n",
    "    csv_files = [os.path.join(path,file) for file in files if file.endswith('.csv')]\n",
    "    dfs = [pd.read_csv(file_path) for file_path in csv_files]\n",
    "    \n",
    "    df = pd.concat(dfs,ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_new = \"./scenarios_with_labels\"\n",
    "path_old_labelled = \"./old_dataset/labelled/\"\n",
    "path_old_unlabelled = \"./old_dataset/unlabelled/\"\n",
    "\n",
    "new_dataset_path = './dataset_new.csv'\n",
    "old_dataset_path = './dataset_old.csv'\n",
    "\n",
    "df_new = combine_scenarios(path_new)\n",
    "df_new.to_csv(new_dataset_path, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labelled = combine_scenarios(path_old_labelled)\n",
    "df_unlabelled = combine_scenarios(path_old_unlabelled)\n",
    "\n",
    "df_old = pd.concat([df_labelled , df_unlabelled])\n",
    "\n",
    "# df_old.to_csv(old_dataset_path, index = False)\n",
    "\n",
    "df_old.to_csv(old_dataset_path ,index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign label to old data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scenario4.csv', 'scenario5.csv', 'scenario6.csv', 'scenario2.csv', 'scenario3.csv', 'scenario1.csv']\n"
     ]
    }
   ],
   "source": [
    "scenarios_examples_path = \"./scenarios_examples_multi_tasks_output_labels\"\n",
    "scenarios_versions_path = \"./scenarios_versions_csv_format_old\"\n",
    "\n",
    "scenarios_with_labels = \"./scenarios_with_labels_old\"\n",
    "\n",
    "assign_labels_from_scenarios_examples(scenarios_examples_path, scenarios_versions_path, scenarios_with_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign label to generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scenario4.csv', 'scenario5.csv', 'scenario6.csv', 'scenario2.csv', 'scenario3.csv', 'scenario1.csv']\n"
     ]
    }
   ],
   "source": [
    "scenarios_examples_path = \"./scenarios_examples_multi_tasks_output_labels\"\n",
    "scenarios_versions_path = \"./scenarios_generated/paragraph_wise_2/\"\n",
    "\n",
    "scenarios_with_labels = \"./scenarios_with_labels_generated_split/\"\n",
    "\n",
    "assign_labels_from_scenarios_examples(scenarios_examples_path, scenarios_versions_path, scenarios_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios_examples_path = \"./scenarios_examples_multi_tasks_output_labels\"\n",
    "scenarios_versions_path = \"./scenarios_generated/paragraph_wise/\"\n",
    "\n",
    "scenarios_with_labels = \"./scenarios_with_labels_generated/\"\n",
    "\n",
    "assign_labels_from_scenarios_examples(scenarios_examples_path, scenarios_versions_path, scenarios_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scenario4.csv', 'scenario5.csv', 'scenario6.csv', 'scenario2.csv', 'scenario3.csv', 'scenario1.csv']\n"
     ]
    }
   ],
   "source": [
    "scenarios_examples_path = \"./scenarios_examples_multi_tasks_output_labels\"\n",
    "scenarios_versions_path = \"./experiment/test/\"\n",
    "\n",
    "scenarios_with_labels = \"./experiment/test_label\"\n",
    "\n",
    "assign_labels_from_scenarios_examples(scenarios_examples_path, scenarios_versions_path, scenarios_with_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined different dataset, find the duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates_in_each_scenarios(path):\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    csv_files = [os.path.join(path,file) for file in files if file.endswith('.csv')]\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file_path)\n",
    "        print(len(df), len(df.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./old_dataset/unlabelled/scenario4.csv\n",
      "20 5\n",
      "./old_dataset/unlabelled/scenario5.csv\n",
      "20 11\n",
      "./old_dataset/unlabelled/scenario6.csv\n",
      "20 11\n",
      "./old_dataset/unlabelled/scenario2.csv\n",
      "20 10\n",
      "./old_dataset/unlabelled/scenario3.csv\n",
      "20 12\n",
      "./old_dataset/unlabelled/scenario1.csv\n",
      "20 18\n"
     ]
    }
   ],
   "source": [
    "duplicates_in_each_scenarios(path_old_unlabelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_scenarios_from_different_sources(path1, path2, output_directory):\n",
    "    \n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    files1 = os.listdir(path1)\n",
    "    files2 = os.listdir(path2)\n",
    "\n",
    "\n",
    "    csv_files1 = [file for file in files1 if file.endswith('.csv')]\n",
    "    csv_files2 = [file for file in files2 if file.endswith('.csv')]\n",
    "\n",
    "    for file_path1 in csv_files1:\n",
    "        for file_path2 in csv_files2:\n",
    "            \n",
    "            if file_path1==file_path2:\n",
    "                print(file_path1,file_path2)\n",
    "                df1 = pd.read_csv(os.path.join(path1,file_path1))\n",
    "                df2 = pd.read_csv(os.path.join(path2,file_path2))\n",
    "                df = pd.concat([df1['Scenario'],df2['Scenario']], ignore_index=True)\n",
    "                \n",
    "               \n",
    "                print(len(df), len(df.drop_duplicates()))\n",
    "\n",
    "                df.drop_duplicates().to_csv(os.path.join(output_directory,file_path1),index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_directory = \"./scenarios_versions_csv_format_old/\"\n",
    "# combine_scenarios_from_different_sources(path_old_labelled,path_old_unlabelled, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_from_old_scenarios(path1 , path2):\n",
    "\n",
    "    files1 = os.listdir(path1)\n",
    "    files2 = os.listdir(path2)\n",
    "\n",
    "\n",
    "    csv_files1 = [file for file in files1 if file.endswith('.csv')]\n",
    "    csv_files2 = [file for file in files2 if file.endswith('.csv')]\n",
    "\n",
    "    for file_path1 in csv_files1:\n",
    "        for file_path2 in csv_files2:\n",
    "            \n",
    "            if file_path1==file_path2:\n",
    "                print(file_path1,file_path2)\n",
    "\n",
    "                df1 = pd.read_csv(os.path.join(path1,file_path1))\n",
    "                df2 = pd.read_csv(os.path.join(path2,file_path2))\n",
    "\n",
    "            \n",
    "                df = pd.concat([df1['Scenario'],df2['Scenario']], ignore_index=True)\n",
    "                print(\"Combined datapoint: \")\n",
    "                print(f\"Original: {len(df)}, Unique: {len(df.drop_duplicates())}\")\n",
    "\n",
    "                df = df2[~df2.Scenario.isin(df1.Scenario)]\n",
    "                \n",
    "                df.to_csv(os.path.join(path2,file_path2), index = False)\n",
    "                \n",
    "                print(\"Old datapoint\")\n",
    "                print(f\"Original: {len(df)}, Unique: {len(df.drop_duplicates())}\")\n",
    "\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario4.csv scenario4.csv\n",
      "Combined datapoint: \n",
      "Original: 60, Unique: 60\n",
      "Old datapoint\n",
      "Original: 20, Unique: 20\n",
      "\n",
      "scenario5.csv scenario5.csv\n",
      "Combined datapoint: \n",
      "Original: 60, Unique: 60\n",
      "Old datapoint\n",
      "Original: 20, Unique: 20\n",
      "\n",
      "scenario6.csv scenario6.csv\n",
      "Combined datapoint: \n",
      "Original: 60, Unique: 60\n",
      "Old datapoint\n",
      "Original: 20, Unique: 20\n",
      "\n",
      "scenario2.csv scenario2.csv\n",
      "Combined datapoint: \n",
      "Original: 60, Unique: 60\n",
      "Old datapoint\n",
      "Original: 20, Unique: 20\n",
      "\n",
      "scenario3.csv scenario3.csv\n",
      "Combined datapoint: \n",
      "Original: 60, Unique: 60\n",
      "Old datapoint\n",
      "Original: 20, Unique: 20\n",
      "\n",
      "scenario1.csv scenario1.csv\n",
      "Combined datapoint: \n",
      "Original: 60, Unique: 60\n",
      "Old datapoint\n",
      "Original: 20, Unique: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path1 = \"./scenarios_versions_csv_format\"\n",
    "path2 = \"./scenarios_versions_csv_format_old/\"\n",
    "remove_duplicates_from_old_scenarios(path1 , path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_old_extracted = './scenarios_with_labels_split/train'\n",
    "\n",
    "df = combine_scenarios(path_old_extracted)\n",
    "\n",
    "df.to_csv(\"./train_new.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_generated = './scenarios_with_labels_split/'\n",
    "\n",
    "\n",
    "# df = combine_scenarios(path_generated)\n",
    "\n",
    "# df.to_csv(\"train_generated.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = pd.read_csv(path_generated+\"dataset_old.csv\")\n",
    "df_new = pd.read_csv(path_generated+\"train_new.csv\")\n",
    "df_generated = pd.read_csv(path_generated+\"train_generated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 120)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_old[\"Scenario\"]), len(df_old[\"Scenario\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 168\n"
     ]
    }
   ],
   "source": [
    "print(len(df_new), len(df_new.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120\n"
     ]
    }
   ],
   "source": [
    "print(len(df_old), len(df_old.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680 1680\n"
     ]
    }
   ],
   "source": [
    "print(len(df_generated), len(df_generated.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes\n",
    "df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "df_combined = pd.concat([df_combined, df_generated], ignore_index=True)\n",
    "df_combined.to_csv(path_generated+\"train.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1968 1968\n"
     ]
    }
   ],
   "source": [
    "print(len(df_combined), len(df_combined.drop_duplicates()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup = df_combined.duplicated()\n",
    "dup.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(path1, path2, path3, train = 28):\n",
    "    os.makedirs(path2, exist_ok=True)\n",
    "    os.makedirs(path3, exist_ok=True)\n",
    "\n",
    "    files1 = os.listdir(path1)\n",
    "    csv_files1 = [file for file in files1 if file.endswith('.csv')]\n",
    "\n",
    "    for file_path1 in csv_files1:\n",
    "        df1 = pd.read_csv(os.path.join(path1,file_path1))\n",
    "        df1[:train].to_csv(os.path.join(path2,file_path1), index= False)\n",
    "        df1[train:].to_csv(os.path.join(path3,file_path1), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"./scenarios_with_labels_split/test/\"\n",
    "path2 = \"./scenarios_with_labels_split/val/\"\n",
    "path3 = \"./scenarios_with_labels_split/test/\"\n",
    "train_test_split(path1, path2, path3, train = 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_old_extracted = './scenarios_with_labels_first32/train/'\n",
    "\n",
    "\n",
    "df = combine_scenarios(path_old_extracted)\n",
    "\n",
    "df.to_csv(\"./dataset_new_first32.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove labeled of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_label(path1, path2):\n",
    "    os.makedirs(path2, exist_ok=True)\n",
    "\n",
    "    files1 = os.listdir(path1)\n",
    "    csv_files1 = [file for file in files1 if file.endswith('.csv')]\n",
    "\n",
    "    for file_path1 in csv_files1:\n",
    "\n",
    "        df1 = pd.read_csv(os.path.join(path1,file_path1))\n",
    "        df1[\"Scenario\"].to_csv(os.path.join(path2,file_path1), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"./scenarios_with_labels_old/\"\n",
    "path2 = \"./scenarios_versions_csv_format_old/\"\n",
    "remove_label(path1 , path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print all directory and folders in the given path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_and_folders(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        print(f\"Root: {root}\")\n",
    "        for dir_name in dirs:\n",
    "            print(f\" Directory: {dir_name}\")\n",
    "        for file_name in files:\n",
    "            print(f\" File: {file_name}\")\n",
    "\n",
    "# Specify the directory you want to list\n",
    "directory_path = './'\n",
    "list_files_and_folders(directory_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
